import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import pickle

# Fonction pour g√©n√©rer une onde ECG simul√©e
def ecg_synthetique(t):
    """G√©n√®re un signal ECG synth√©tique"""
    return (
        0.1 * np.sin(2 * np.pi * t * 1) +  # Onde P
        -0.15 * np.exp(-((t - 0.25) ** 2) / 0.001) +  # Onde Q
        1.0 * np.exp(-((t - 0.3) ** 2) / 0.0005) +  # Pic R
        -0.2 * np.exp(-((t - 0.35) ** 2) / 0.001) +  # Onde S
        0.3 * np.exp(-((t - 0.5) ** 2) / 0.01)  # Onde T
    )

def generer_donnees_entrainement(nb_cycles=50, points_par_cycle=500):
    """
    G√©n√®re des donn√©es d'entra√Ænement avec variations
    
    Args:
        nb_cycles: nombre de cycles √† g√©n√©rer
        points_par_cycle: r√©solution du signal
    
    Returns:
        signal complet pour l'entra√Ænement
    """
    print(f"üìä G√©n√©ration de {nb_cycles} cycles ECG pour l'entra√Ænement...")
    
    t = np.linspace(0, 1, points_par_cycle)
    cycles = []
    
    for i in range(nb_cycles):
        # G√©n√©rer le cycle de base
        cycle = ecg_synthetique(t)
        
        # Ajouter des variations pour rendre le mod√®le plus robuste
        # Variation d'amplitude (¬±10%)
        amplitude_var = np.random.uniform(0.9, 1.1)
        cycle = cycle * amplitude_var
        
        # Ajout de bruit
        bruit = 0.03 * np.random.normal(size=cycle.shape)
        cycle = cycle + bruit
        
        cycles.append(cycle)
    
    signal = np.concatenate(cycles)
    print(f"‚úÖ Signal g√©n√©r√©: {len(signal)} points")
    
    return signal

def normaliser_signal(signal):
    """Normalise le signal sur [0, 1] et retourne les bornes."""
    min_val = float(np.min(signal))
    max_val = float(np.max(signal))
    if np.isclose(max_val, min_val):
        raise ValueError("Amplitude du signal nulle, impossible de normaliser")
    signal_norm = (signal - min_val) / (max_val - min_val)
    return signal_norm, min_val, max_val

def denormaliser_signal(signal_norm, min_val, max_val):
    """Restaure l'√©chelle originale d'un signal normalis√©."""
    return signal_norm * (max_val - min_val) + min_val

def preparer_sequences(signal, sequence_length=50):
    """
    Pr√©pare les s√©quences pour l'apprentissage LSTM
    
    Args:
        signal: signal ECG complet
        sequence_length: longueur de la s√©quence d'entr√©e
    
    Returns:
        X: s√©quences d'entr√©e
        y: valeurs √† pr√©dire
    """
    print(f"\nüîÑ Pr√©paration des s√©quences (longueur={sequence_length})...")
    
    X, y = [], []
    
    for i in range(len(signal) - sequence_length):
        X.append(signal[i:i + sequence_length])
        y.append(signal[i + sequence_length])
    
    X = np.array(X)
    y = np.array(y)
    
    # Reshape pour LSTM: (samples, timesteps, features)
    X = X.reshape((X.shape[0], X.shape[1], 1))
    
    print(f"‚úÖ Forme X: {X.shape}")
    print(f"‚úÖ Forme y: {y.shape}")
    
    return X, y

def creer_modele_lstm(sequence_length=50):
    """
    Cr√©e l'architecture du mod√®le LSTM
    
    Args:
        sequence_length: longueur des s√©quences d'entr√©e
    
    Returns:
        mod√®le LSTM compil√©
    """
    print("\nüèóÔ∏è  Construction du mod√®le LSTM...")
    
    model = Sequential([
        # Premi√®re couche LSTM avec 128 unit√©s
        LSTM(128, input_shape=(sequence_length, 1), return_sequences=True),
        Dropout(0.2),  # √âvite le surapprentissage
        
        # Deuxi√®me couche LSTM avec 64 unit√©s
        LSTM(64, return_sequences=False),
        Dropout(0.2),
        
        # Couche Dense pour la pr√©diction
        Dense(32, activation='relu'),
        Dense(1)  # Sortie: pr√©diction d'un seul point
    ])
    
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    
    print("\nüìã Architecture du mod√®le:")
    model.summary()
    
    return model

def entrainer_modele(model, X_train, y_train, epochs=50, batch_size=64):
    """
    Entra√Æne le mod√®le LSTM
    
    Args:
        model: mod√®le √† entra√Æner
        X_train, y_train: donn√©es d'entra√Ænement
        epochs: nombre d'√©poques
        batch_size: taille des batches
    
    Returns:
        historique de l'entra√Ænement
    """
    print("\nüöÄ D√©but de l'entra√Ænement...\n")
    
    # Callback pour arr√™ter si le mod√®le ne s'am√©liore plus
    early_stopping = EarlyStopping(
        monitor='loss',
        patience=5,
        restore_best_weights=True
    )
    
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=0.2,
        callbacks=[early_stopping],
        verbose=1
    )
    
    print("\n‚úÖ Entra√Ænement termin√©!")
    
    return history

def visualiser_entrainement(history):
    """Visualise les courbes d'apprentissage"""
    plt.figure(figsize=(12, 4))
    
    # Perte
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Loss (entra√Ænement)')
    plt.plot(history.history['val_loss'], label='Loss (validation)')
    plt.title('√âvolution de la perte')
    plt.xlabel('√âpoque')
    plt.ylabel('MSE')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Erreur absolue moyenne
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='MAE (entra√Ænement)')
    plt.plot(history.history['val_mae'], label='MAE (validation)')
    plt.title('Erreur absolue moyenne')
    plt.xlabel('√âpoque')
    plt.ylabel('MAE')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('lstm_training_history.png', dpi=300, bbox_inches='tight')
    print("\nüíæ Graphique sauvegard√©: lstm_training_history.png")
    plt.show()

def tester_modele(model, X_test, y_test, min_val, max_val):
    """Teste le mod√®le et visualise les pr√©dictions"""
    print("\nüß™ Test du mod√®le...")
    
    y_pred = model.predict(X_test, verbose=0)
    
    # Revenir √† l'√©chelle d'origine pour analyser la qualit√© r√©elle
    y_pred_denorm = denormaliser_signal(y_pred.flatten(), min_val, max_val)
    y_test_denorm = denormaliser_signal(y_test, min_val, max_val)
    
    mse = np.mean((y_test_denorm - y_pred_denorm) ** 2)
    mae = np.mean(np.abs(y_test_denorm - y_pred_denorm))
    
    print(f"üìä MSE: {mse:.6f}")
    print(f"üìä MAE: {mae:.6f}")
    
    # Visualisation
    plt.figure(figsize=(14, 5))
    
    nb_points = 1000
    plt.plot(y_test_denorm[:nb_points], label='Signal r√©el', linewidth=2)
    plt.plot(y_pred_denorm[:nb_points], label='Pr√©diction LSTM', 
             linestyle='--', linewidth=2, alpha=0.8)
    plt.title('Pr√©diction LSTM vs Signal R√©el', fontsize=14, fontweight='bold')
    plt.xlabel('Index temporel')
    plt.ylabel('Amplitude (mV)')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('lstm_prediction_test.png', dpi=300, bbox_inches='tight')
    print("üíæ Graphique sauvegard√©: lstm_prediction_test.png")
    plt.show()

def sauvegarder_modele(model, sequence_length, min_val, max_val):
    """Sauvegarde le mod√®le entra√Æn√©"""
    model.save('ecg_lstm_model.h5')
    
    # Sauvegarder aussi les param√®tres
    params = {
        'sequence_length': sequence_length,
        'signal_min': min_val,
        'signal_max': max_val,
    }
    with open('model_params.pkl', 'wb') as f:
        pickle.dump(params, f)
    
    print("\nüíæ Mod√®le sauvegard√©:")
    print("   - ecg_lstm_model.h5")
    print("   - model_params.pkl")

def main():
    """Fonction principale"""
    print("=" * 70)
    print("   ENTRA√éNEMENT DU MOD√àLE LSTM POUR RECONSTRUCTION ECG")
    print("=" * 70)
    
    # Param√®tres
    NB_CYCLES = 50  # Nombre de cycles pour l'entra√Ænement
    POINTS_PAR_CYCLE = 500
    SEQUENCE_LENGTH = 50  # Longueur de la s√©quence pour pr√©dire le point suivant
    EPOCHS = 30
    BATCH_SIZE = 64
    
    # 1. G√©n√©rer les donn√©es d'entra√Ænement
    signal = generer_donnees_entrainement(NB_CYCLES, POINTS_PAR_CYCLE)
    signal_norm, signal_min, signal_max = normaliser_signal(signal)
    
    # 2. Pr√©parer les s√©quences
    X, y = preparer_sequences(signal_norm, SEQUENCE_LENGTH)
    
    # 3. Diviser en train/test
    split_idx = int(0.8 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    print(f"\nüìä Donn√©es d'entra√Ænement: {len(X_train)} s√©quences")
    print(f"üìä Donn√©es de test: {len(X_test)} s√©quences")
    
    # 4. Cr√©er le mod√®le
    model = creer_modele_lstm(SEQUENCE_LENGTH)
    
    # 5. Entra√Æner le mod√®le
    history = entrainer_modele(model, X_train, y_train, EPOCHS, BATCH_SIZE)
    
    # 6. Visualiser l'entra√Ænement
    visualiser_entrainement(history)
    
    # 7. Tester le mod√®le
    tester_modele(model, X_test, y_test, signal_min, signal_max)
    
    # 8. Sauvegarder le mod√®le
    sauvegarder_modele(model, SEQUENCE_LENGTH, signal_min, signal_max)
    
    print("\n" + "=" * 70)
    print("‚úÖ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS!")
    print("=" * 70)
    print("\nVous pouvez maintenant utiliser le mod√®le pour reconstruire")
    print("les signaux ECG √©chantillonn√©s re√ßus via MQTT.")

if __name__ == "__main__":
    main()